# ðŸŽ¬ MovieLens End-to-End ETL & Analysis Pipeline

**Author:** Ogechukwu Ezenwa  
**Date:** November 4, 2025  
**Course:** IDS 706 â€“ Data Engineering Systems  
**Assignment:** Data Orchestration with Airflow
---

## Project Overview
This project implements a complete **data engineering pipeline** using **Apache Airflow**.  
It automates the ingestion, transformation, merging, loading, and analysis of the **MovieLens** dataset.  
The pipeline demonstrates orchestration, task parallelism, database integration, and lightweight machine learning in an Airflow DAG.

**Goal:**  
To build a reproducible workflow that prepares movie and rating data for analytical modeling while showcasing best practices in modern ETL pipeline design.

---

## Project Struture
```bash
.
â”œâ”€â”€ config
â”‚   â””â”€â”€ airflow.cfg                          # configuration setting for apache airflow
â”œâ”€â”€ dag_success                              # expected output when dag run successfully
â”‚   â”œâ”€â”€ dag_Airflow_UI.png
â”‚   â”œâ”€â”€ dag_graph.png
â”‚   â””â”€â”€ movielens_spark_etl_ml-graph.png
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ movielens_end_to_end_parallel.py     # Main DAG file
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                                 # Raw MovieLens data
â”‚   â”œâ”€â”€ tmp/                                 # Intermediate parquet files
â”‚   â”œâ”€â”€ processed/                           # Final merged datasets
â”‚   â””â”€â”€ artifacts/                           # Model outputs & plots
â”œâ”€â”€ .devcontainer/
â”‚   â”œâ”€â”€ docker-compose.yml                   # Container orchestration setup
â”‚   â””â”€â”€ .Dockerfile                          # Custom Airflow image
â”œâ”€â”€ plugins/                                 # (Optional) custom Airflow plugins
â”œâ”€â”€ logs/                                    # Airflow logs
â”œâ”€â”€ requirements.txt                         # packages to install
â””â”€â”€ README.md

    
```
## Pipeline Stages

| Stage | Description | Tool/Method |
|---|---|---|
| **1. Ingest** | Download & extract MovieLens zip to `data/raw/<ds_nodash>/`. | `requests`, Airflow `@task` |
| **2. Transform (Parallel)** | Two branches run concurrently under a `TaskGroup`: <br>â€¢ **Movies (pandas):** extract `year` from title, one-hot top genres â†’ `data/tmp/movies_clean.parquet` <br>â€¢ **Ratings (PySpark):** filter 0.5â€“5.0, drop `timestamp`, aggregate per movie (`n_ratings`, `rating_mean`) â†’ `data/tmp/ratings_clean.parquet` | **pandas** (movies), **PySpark** (ratings), Airflow `TaskGroup` |
| **3. Merge (Spark) & Load** | Spark joins movies + ratings on `movieId`, writes `data/processed/merged_<ds>.parquet`; pandas reads that and loads table **`movies_features`** into Postgres. | **PySpark** (merge), **pandas + SQLAlchemy + psycopg2** (load) |
| **4. Analyze** | Read `movies_features` from Postgres; train **LinearRegression** to predict `rating_mean` from `year` + genre dummies; save plot & metrics. | `scikit-learn`, `matplotlib` |
| **5. Cleanup** | Remove extracted raw folder and any `.parquet` in `data/tmp/`; keep processed data & artifacts. | `shutil`, Airflow `@task` |

## Setup & Run

### 1. Clone the repository
```bash
cd <directory>
git clone git@github.com:Gechyb/IDS706_DE_Wk10_Airflow_pipeline.git
```

**Technologies Used**
- **Apache Airflow** â€“ Orchestration and scheduling  
- **PostgreSQL** â€“ Data storage backend  
- **Docker Compose** â€“ Containerized environment  
- **pandas / scikit-learn** â€“ Data wrangling and ML analysis  
- **SQLAlchemy** â€“ Database connection layer  
- **Celery + Redis** â€“ Parallel task execution 

### 2. Build and start services

```bash
docker compose -f .devcontainer/docker-compose.yml build
docker compose -f .devcontainer/docker-compose.yml up airflow-init
docker compose -f .devcontainer/docker-compose.yml up -d
```
Login into http://localhost:8080/ 
Username:airflow
Password:airflow

### 3. Access the Airflow Web UI

- URL: http://localhost:8080
- Username: airflow
- Password: airflow

### 4. Trigger the DAG

- DAG ID: movielens_end_to_end_parallel
- In the Airflow Web UI:
    1. Unpause the DAG
    2. Trigger it manually
    3. Wait for all tasks to complete (they should turn green âœ…)

## Screenshots
Successful DAG Execution

Screenshot of the Grid View showing all green (success) tasks below:

![alt text](dag_success/dag_Airflow_UI.png)

DAG Graph View

Screenshot of the Graph View showing the full workflow below:
![alt text](dag_success/dag_graph.png)
